{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_AUTH_TOKEN = \"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\"\n",
    "headers = {\"Authorisation\" : \"token {}\".format(GITHUB_AUTH_TOKEN)}\n",
    "GITHUB_ISSUES_URL = \"https://api.github.com/repos/shosetsuorg/shosetsu/issues\"\n",
    "GITHUB_COMMITS_URL = \"https://api.github.com/repos/shosetsuorg/shosetsu/commits\"\n",
    "# data=json.dumps(data)\n",
    "issues_filter = {\"state\":\"closed\"}\n",
    "issues_data = requests.get(GITHUB_ISSUES_URL, params=issues_filter)\n",
    "commits_data = requests.get(GITHUB_COMMITS_URL)\n",
    "# print(r)\n",
    "issues = json.loads(issues_data.text)\n",
    "commits = json.loads(commits_data.text)\n",
    "# print(output[0][\"commit\"][\"author\"][\"date\"])\n",
    "\n",
    "issue_commit_map = {}\n",
    "\n",
    "for issue in issues:\n",
    "    issue_created_at = issue[\"created_at\"]\n",
    "    issue_closed_at = issue[\"closed_at\"]\n",
    "    issue_created_at = datetime.fromisoformat(issue_created_at.replace('Z', '+00:00'))\n",
    "    issue_closed_at = datetime.fromisoformat(issue_closed_at.replace('Z', '+00:00'))\n",
    "    issue_commit_map[issue[\"id\"]] = {\"issue_title\": issue[\"title\"], \"commit_messages\":[]}\n",
    "    for commit in commits:\n",
    "    # print(\"created_at\",obj[\"created_at\"], \"\\nclosed_at\", obj[\"closed_at\"])\n",
    "        commit_made_at = commit[\"commit\"][\"committer\"][\"date\"]\n",
    "        commit_made_at = datetime.fromisoformat(commit_made_at.replace('Z', '+00:00'))\n",
    "        # print(commit_made_at)\n",
    "        if commit_made_at>=issue_created_at and commit_made_at<=issue_closed_at:\n",
    "            # new_item = {\"issue_id\": issue[\"id\"], \"commit_message\":commit[\"commit\"][\"message\"]}\n",
    "            issue_commit_map[issue[\"id\"]][\"commit_messages\"].append(commit[\"commit\"][\"message\"])\n",
    "\n",
    "# print(issue_commit_map)\n",
    "\n",
    "issue_commit_map_list = list(issue_commit_map.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ISSUE BODY IS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_AUTH_TOKEN = \"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\"\n",
    "headers = {\"Authorisation\" : \"token {}\".format(GITHUB_AUTH_TOKEN)}\n",
    "GITHUB_ISSUES_URL = \"https://api.github.com/repos/shosetsuorg/shosetsu/issues\"\n",
    "GITHUB_COMMITS_URL = \"https://api.github.com/repos/shosetsuorg/shosetsu/commits\"\n",
    "# data=json.dumps(data)\n",
    "issues_filter = {\"state\":\"closed\"}\n",
    "issues_data = requests.get(GITHUB_ISSUES_URL, params=issues_filter)\n",
    "commits_data = requests.get(GITHUB_COMMITS_URL)\n",
    "# print(r)\n",
    "issues = json.loads(issues_data.text)\n",
    "commits = json.loads(commits_data.text)\n",
    "# print(output[0][\"commit\"][\"author\"][\"date\"])\n",
    "\n",
    "issue_commit_map = {}\n",
    "\n",
    "for issue in issues:\n",
    "    issue_created_at = issue[\"created_at\"]\n",
    "    issue_closed_at = issue[\"closed_at\"]\n",
    "    issue_created_at = datetime.fromisoformat(issue_created_at.replace('Z', '+00:00'))\n",
    "    issue_closed_at = datetime.fromisoformat(issue_closed_at.replace('Z', '+00:00'))\n",
    "    issue_commit_map[issue[\"id\"]] = {\"issue_body\": issue[\"body\"], \"commit_messages\":[]}\n",
    "    for commit in commits:\n",
    "    # print(\"created_at\",obj[\"created_at\"], \"\\nclosed_at\", obj[\"closed_at\"])\n",
    "        commit_made_at = commit[\"commit\"][\"committer\"][\"date\"]\n",
    "        commit_made_at = datetime.fromisoformat(commit_made_at.replace('Z', '+00:00'))\n",
    "        # print(commit_made_at)\n",
    "        if commit_made_at>=issue_created_at and commit_made_at<=issue_closed_at:\n",
    "            # new_item = {\"issue_id\": issue[\"id\"], \"commit_message\":commit[\"commit\"][\"message\"]}\n",
    "            issue_commit_map[issue[\"id\"]][\"commit_messages\"].append(commit[\"commit\"][\"message\"])\n",
    "\n",
    "# print(issue_commit_map)\n",
    "\n",
    "issue_commit_map_list = list(issue_commit_map.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fix padding', 'Fix enabled parameter on switch settings', 'MatchFullscreen is false by default', 'Address comment', 'Match fullscreen to focus option', 'Add fullscreen option', 'Clean up\\n\\nHandle various build warnings and removing older classes', 'Update dependencies', 'Fix animation stutter', 'Re-add elevation and remove top content padding', 'Fix cover ContentScale', 'Fix expanded cover having a ratio and being rounded', 'NovelCard fixes, forced center alignment and outside padding', 'Fix scrolling filters', 'Add unread badge click', 'Licenses', 'Use a standard cover ratio', 'UI Cleanup and improvements']\n",
      "string2:  Fix padding, Fix enabled parameter on switch settings, MatchFullscreen is false by default, Address comment, Match fullscreen to focus option, Add fullscreen option, Clean up  Handle various build warnings and removing older classes, Update dependencies, Fix animation stutter, Re-add elevation and remove top content padding, Fix cover ContentScale, Fix expanded cover having a ratio and being rounded, NovelCard fixes, forced center alignment and outside padding, Fix scrolling filters, Add unread badge click, Licenses, Use a standard cover ratio, UI Cleanup and improvements\n",
      "string1:   Add a setting to disable fullscreen,  Add a setting to match fullscreen to focus mode\n",
      "Semantic similarity score: 0.7655267620196826\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the medium-sized English model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def semantic_similarity(str1, str2):\n",
    "    \"\"\"\n",
    "    Calculate semantic similarity between two strings using spaCy.\n",
    "    \n",
    "    Args:\n",
    "    - str1 (str): First string.\n",
    "    - str2 (str): Second string.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Similarity score between 0 and 1. \n",
    "             1 means the strings are semantically identical, \n",
    "             0 means they have no semantic similarity.\n",
    "    \"\"\"\n",
    "    # Process the strings\n",
    "    doc1 = nlp(str1)\n",
    "    doc2 = nlp(str2)\n",
    "    \n",
    "    # Calculate semantic similarity\n",
    "    similarity_score = doc1.similarity(doc2)\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "# Example usage:\n",
    "# string1 = \"**Describe the bug**\\r\\n\\r\\nMy journal is being spammed with:\\r\\n\\r\\n```\\r\\nAug 24 11:24:21 pc.example.com gnome-shell[1840277]: g_signal_handler_disconnect: assertion 'handler_id > 0' failed\\r\\nAug 24 11:24:21 pc.example.com gnome-shell[1840277]: g_signal_handler_disconnect: assertion 'handler_id > 0' failed\\r\\nAug 24 11:24:21 pc.example.com gnome-shell[1840277]: g_signal_handler_disconnect: assertion 'handler_id > 0' failed\\r\\nAug 24 11:24:21 pc.example.com gnome-shell[1840277]: g_signal_handler_disconnect: assertion 'handler_id > 0' failed\\r\\nAug 24 11:24:21 pc.example.com gnome-shell[1840277]: g_signal_handler_disconnect: assertion 'handler_id > 0' failed\\r\\nAug 24 11:24:21 pc.example.com gnome-shell[1840277]: g_signal_handler_disconnect: assertion 'handler_id > 0' failed\\r\\nAug 24 11:24:21 pc.example.com gnome-shell[1840277]: g_signal_handler_disconnect: assertion 'handler_id > 0' failed\\r\\nAug 24 11:24:21 pc.example.com gnome-shell[1840277]: g_signal_handler_disconnect: assertion 'handler_id > 0' failed\\r\\nAug 24 11:24:21 pc.example.com gnome-shell[1840277]: g_signal_handler_disconnect: assertion 'handler_id > 0' failed\\r\\nAug 24 11:24:21 pc.example.com gnome-shell[1840277]: g_signal_handler_disconnect: assertion 'handler_id > 0' failed\\r\\nAug 24 11:24:21 pc.example.com gnome-shell[1840277]: g_signal_handler_disconnect: assertion 'handler_id > 0' failed\\r\\n\\r\\n```\\r\\n\\r\\n**Steps To Reproduce:**\\r\\n\\r\\nNot really sure of any specific steps, but enabling/disabling GSConnect makes this happen/not happen.\\r\\n\\r\\n**Expected behavior**\\r\\n\\r\\nShould not be spamming the journal with an assertion.\\r\\n\\r\\n**System Details (please complete the following information):**\\r\\n\\r\\n - **GSConnect version:** 50\\r\\n   - **Installed from:** Fedora 36 RPM\\r\\n - **GNOME/Shell version:** 42.4\\r\\n - **Distro/Release:** Fedora 36\\r\\n\\r\\n**GSConnect environment (if applicable):**\\r\\n\\r\\n - **Paired Device(s):** Lineage 19.1 phone and Honor 8\\r\\n - **KDE Connect app version:** 1.19.1\\r\\n - **Plugin(s):** N/A\"\n",
    "# string2 = \"Since GNOME 42, the popup menu uses `connectObject()` instead of storing\\r\\nthe handler ID on the item.\\r\\n\\r\\nGet the handler ID with `GObject.signal_handler_find()` and connect the\\r\\nreplacement with `connectObject()`.\\r\\n\\r\\nfixes #1442\"\n",
    "\n",
    "# string1 = issue_commit_map_list[3][\"issue_title\"]\n",
    "string1 = issue_commit_map_list[3][\"issue_body\"]\n",
    "string2 = issue_commit_map_list[3][\"commit_messages\"]\n",
    "print(string2)\n",
    "string2 = \", \".join(string2)\n",
    "string2 = string2.split('\\n')\n",
    "string2 = \" \".join(string2)\n",
    "string2 = string2.split('\\t')\n",
    "string2 = \" \".join(string2)\n",
    "# print(\"string1: \", string1)\n",
    "# string1.split(['[',']'], string1)\n",
    "delimiters = ['[',']']\n",
    "# string = \"\"\n",
    "for delimiter in delimiters:\n",
    "    string1 = \"  \".join(string1.split(delimiter))\n",
    "string = string1.split(\"  \")\n",
    "string1 = [string[temp] for temp in range(len(string)) if temp%2 == 1]\n",
    "# print(string1)\n",
    "print(\"string2: \", string2)\n",
    "fin_str1 = \"\"\n",
    "fin_str1 = \", \".join(string1)\n",
    "print(\"string1: \", fin_str1)\n",
    "similarity = semantic_similarity(fin_str1, string2)\n",
    "print(f\"Semantic similarity score: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key ' Add a setting to disable fullscreen,  Add a setting to match fullscreen to focus mode' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 40\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m similarity\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# text1 = \"The cat sat on the mat.\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# text2 = \"The feline is resting on the floor covering.\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Use a pre-trained model (replace with your path)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# model_path = \"path/to/your/model.bin\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m similarity \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfin_str1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimilarity score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, similarity)\n",
      "Cell \u001b[0;32mIn[31], line 25\u001b[0m, in \u001b[0;36mcalculate_similarity\u001b[0;34m(text1, text2, model_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m   model \u001b[38;5;241m=\u001b[39m Word2Vec\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Get word vectors\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m word1_vector \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m word2_vector \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mget_vector(text2)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Calculate cosine similarity\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key ' Add a setting to disable fullscreen,  Add a setting to match fullscreen to focus mode' not present\""
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def calculate_similarity(text1, text2, model_path=None):\n",
    "  \"\"\"\n",
    "  Calculates the semantic similarity between two strings using Word2Vec,\n",
    "  returning a value between 0 and 1.\n",
    "\n",
    "  Args:\n",
    "    text1: The first string.\n",
    "    text2: The second string.\n",
    "    model_path: Path to a pre-trained Word2Vec model (optional).\n",
    "\n",
    "  Returns:\n",
    "    The semantic similarity score between the two strings.\n",
    "  \"\"\"\n",
    "\n",
    "  if not model_path:\n",
    "    # Load a pre-trained Word2Vec model or train one with your data\n",
    "    sentences = [text1.split(), text2.split()]\n",
    "    model = Word2Vec(sentences, min_count=1)\n",
    "  else:\n",
    "    model = Word2Vec.load(model_path)\n",
    "\n",
    "  # Get word vectors\n",
    "  word1_vector = model.wv.get_vector(text1)\n",
    "  word2_vector = model.wv.get_vector(text2)\n",
    "\n",
    "  # Calculate cosine similarity\n",
    "  similarity = model.wv.cosine_similarities(word1_vector, [word2_vector])[0][0]\n",
    "\n",
    "  return similarity\n",
    "\n",
    "# Example usage:\n",
    "# text1 = \"The cat sat on the mat.\"\n",
    "# text2 = \"The feline is resting on the floor covering.\"\n",
    "\n",
    "# Use a pre-trained model (replace with your path)\n",
    "# model_path = \"path/to/your/model.bin\"\n",
    "\n",
    "similarity = calculate_similarity(fin_str1, string2)\n",
    "\n",
    "print(\"Similarity score:\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pardhiv/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix padding, Fix enabled parameter on switch settings, MatchFullscreen is false by default, Address comment, Match fullscreen to focus option, Add fullscreen option, Clean up  Handle various build warnings and removing older classes, Update dependencies, Fix animation stutter, Re-add elevation and remove top content padding, Fix cover ContentScale, Fix expanded cover having a ratio and being rounded, NovelCard fixes, forced center alignment and outside padding, Fix scrolling filters, Add unread badge click, Licenses, Use a standard cover ratio, UI Cleanup and improvements\n",
      "Semantic similarity score: 0.30049577355384827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample sentences for training the Word2Vec model (you can replace this with your own data)\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A fast fox jumps over a lazy dog.\",\n",
    "    \"The lazy cat sits on the mat.\",\n",
    "    \"Dogs and cats are both pets.\",\n",
    "    \"The sun rises in the east.\"\n",
    "]\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes and preprocesses the input text.\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text.lower())\n",
    "\n",
    "def semantic_similarity(str1, str2):\n",
    "    \"\"\"\n",
    "    Calculate semantic similarity between two strings using Word2Vec embeddings.\n",
    "    \n",
    "    Args:\n",
    "    - str1 (str): First string.\n",
    "    - str2 (str): Second string.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Similarity score between 0 and 1. \n",
    "             1 means the strings are semantically identical, \n",
    "             0 means they have no semantic similarity.\n",
    "    \"\"\"\n",
    "    # Preprocess the strings\n",
    "    tokens1 = preprocess_text(str1)\n",
    "    tokens2 = preprocess_text(str2)\n",
    "    \n",
    "    # Get word vectors\n",
    "    word_vectors = model.wv\n",
    "    \n",
    "    # Compute cosine similarity between average word vectors of each sentence\n",
    "    similarity_score = word_vectors.n_similarity(tokens1, tokens2)\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "# Example usage:\n",
    "string1 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "# string2 = \"A fast fox jumps over a lazy dog.\"\n",
    "print(string2)\n",
    "similarity = semantic_similarity(fin_str1, string2)\n",
    "print(f\"Semantic similarity score: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.05882352941176472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pardhiv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pardhiv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def calculate_similarity(string1, string2):\n",
    "    # Tokenize the strings into words\n",
    "    words1 = word_tokenize(string1.lower())\n",
    "    words2 = word_tokenize(string2.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words1 = [word for word in words1 if word not in stop_words]\n",
    "    words2 = [word for word in words2 if word not in stop_words]\n",
    "    \n",
    "    # Calculate Jaccard similarity\n",
    "    jaccard_similarity = nltk.jaccard_distance(set(words1), set(words2))\n",
    "    \n",
    "    # Calculate similarity score\n",
    "    similarity_score = 1 - jaccard_similarity\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "# Example usage:\n",
    "string1 = \"Add a setting to disable fullscreen, Add a setting to match fullscreen to focus mode\"\n",
    "string2 = \"Release debug update for r2417, Direct disclaimer to shosetsu website, Prevent concurrent modifies in CatalogViewModel via sync & copy, Add privacy policy button to about, Add more logs to RestoreBackupWorker.kt, Temp fix for backup restore with external repos, Release debug update for r2411, Fix padding, Fix enabled parameter on switch settings, MatchFullscreen is false by default, Address comment, Match fullscreen to focus option, Add fullscreen option, Clean up  Handle various build warnings and removing older classes, Update dependencies, Fix animation stutter, Re-add elevation and remove top content padding, Fix cover ContentScale, Fix expanded cover having a ratio and being rounded, NovelCard fixes, forced center alignment and outside padding, Fix scrolling filters, Add unread badge click, Licenses, Use a standard cover ratio, UI Cleanup and improvements\"\n",
    "similarity_score = calculate_similarity(string1, string2)\n",
    "print(\"Similarity score:\", similarity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "Semantic similarity score: 0.59665143\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download the pre-trained Word2Vec model\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "def calculate_semantic_similarity(string1, string2):\n",
    "    # Tokenize the strings into words\n",
    "    words1 = string1.lower().split()\n",
    "    words2 = string2.lower().split()\n",
    "    \n",
    "    # Filter out words not present in the Word2Vec model\n",
    "    words1 = [word for word in words1 if word in word2vec_model]\n",
    "    words2 = [word for word in words2 if word in word2vec_model]\n",
    "    \n",
    "    # Compute the semantic similarity between the strings\n",
    "    if words1 and words2:\n",
    "        semantic_similarity = word2vec_model.n_similarity(words1, words2)\n",
    "    else:\n",
    "        semantic_similarity = 0.0  # If no common words found, assume zero similarity\n",
    "    \n",
    "    return semantic_similarity\n",
    "\n",
    "# Example usage:\n",
    "string1 = \"Add a setting to disable fullscreen, Add a setting to match fullscreen to focus mode\"\n",
    "string2 = \"Fix animation stutter, Re-add elevation and remove top content padding, Fix cover ContentScale, Fix expanded cover having a ratio and being rounded, NovelCard fixes, forced center alignment and outside padding, Fix scrolling filters, Add unread badge click, Licenses, Use a standard cover ratio, UI Cleanup and improvements\"\n",
    "\n",
    "semantic_similarity = calculate_semantic_similarity(string1, string2)\n",
    "print(\"Semantic similarity score:\", semantic_similarity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
